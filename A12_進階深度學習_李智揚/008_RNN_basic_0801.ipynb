{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sandwhaletree/2023.05_Tibame/blob/main/A12_%E9%80%B2%E9%9A%8E%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92_%E6%9D%8E%E6%99%BA%E6%8F%9A/008_RNN_basic_0801.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1p_KgucRHDv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# batch size (N): 1\n",
        "# Sequence Length (L): 5\n",
        "# feature dimension (H): 8\n",
        "N = 1\n",
        "L = 5\n",
        "DIM_IN = 8\n",
        "\n",
        "# batch first=True\n",
        "inputs = torch.rand(N, L, DIM_IN)\n",
        "\n",
        "# batch first=False\n",
        "inputs = torch.rand(L, N, DIM_IN)\n",
        "inputs.shape"
      ],
      "metadata": {
        "id": "SuudcpxfRfT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM\n",
        "\n",
        "[torch.nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)\n",
        "\n",
        "*   input_size: input dim\n",
        "*   hidden_size: hidden_dim and output_dim\n",
        "*num_layers: number of rnn layers\n",
        "* batch_first:\n",
        "    * True: (N, L, H)\n",
        "    * **False**: (L, N, H)\n",
        "* bidirectional: [False, True]"
      ],
      "metadata": {
        "id": "F7uB2PH1tsMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = 1\n",
        "H_IN = DIM_IN\n",
        "H_HIDDEN = 20\n",
        "LAYERS = 1\n",
        "BIDIRECTIONAL = False\n",
        "D = 1 if not BIDIRECTIONAL else 2\n",
        "\n",
        "lstm = torch.nn.LSTM(input_size=H_IN,\n",
        "                     hidden_size=H_HIDDEN,\n",
        "                     num_layers=LAYERS,\n",
        "                     bidirectional=BIDIRECTIONAL,\n",
        "                     )\n",
        "\n",
        "# Custom h0, c0: init h, c\n",
        "# D: 1 if bidirectional=False else 2\n",
        "h0 = torch.randn(D*LAYERS, N, H_HIDDEN) # (D∗num_layers, N, H_out)\n",
        "c0 = torch.randn(D*LAYERS, N, H_HIDDEN) # (D∗num_layers, N, H_out)\n",
        "output, (hn, cn) = lstm(inputs, (h0, c0))\n",
        "\n",
        "# h0, c0: zero\n",
        "output, (hn, cn) = lstm(inputs)\n",
        "\n",
        "print(output.shape) # (L, N, H_out)\n",
        "print(hn.shape); print(cn.shape)"
      ],
      "metadata": {
        "id": "g6EBBkczuCRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GRU\n",
        "\n",
        "[torch.nn.GRU](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html)\n",
        "\n",
        "*   input_size: input dim\n",
        "*   hidden_size: hidden_dim and output_dim\n",
        "*num_layers: number of rnn layers\n",
        "* batch_first:\n",
        "    * True: (N, L, H)\n",
        "    * False: (L, N, H)\n",
        "* bidirectional: [False, True]"
      ],
      "metadata": {
        "id": "qrd32GGRv-NQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = 1\n",
        "H_IN = DIM_IN\n",
        "H_HIDDEN = 20\n",
        "LAYERS = 2\n",
        "BIDIRECTIONAL = False\n",
        "D = 1 if not BIDIRECTIONAL else 2\n",
        "\n",
        "gru = torch.nn.GRU(input_size=H_IN,\n",
        "                   hidden_size=H_HIDDEN,\n",
        "                   num_layers=LAYERS,\n",
        "                   bidirectional=BIDIRECTIONAL,\n",
        "                   )\n",
        "\n",
        "# Custom h0: init h\n",
        "# D: 1 if bidirectional=False else 2\n",
        "h0 = torch.randn(D*LAYERS, N, H_HIDDEN) # (D∗num_layers, N, H_out)\n",
        "output, hn = gru(inputs, h0)\n",
        "\n",
        "# h0: zero\n",
        "output, hn = gru(inputs)\n",
        "\n",
        "output.shape # (L, N, H_out)"
      ],
      "metadata": {
        "id": "itJQipgrwK6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RNN\n",
        "\n",
        "[torch.nn.RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html?highlight=rnn#torch.nn.RNN)\n",
        "\n",
        "*   input_size: input dim\n",
        "*   hidden_size: hidden_dim and output_dim\n",
        "*num_layers: number of rnn layers\n",
        "* batch_first:\n",
        "    * True: (N, L, H)\n",
        "    * False: (L, N, H)\n",
        "* bidirectional: [False, True]\n",
        "\n"
      ],
      "metadata": {
        "id": "sjBVQvAPR9Ak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnn = torch.nn.RNN(input_size=8,\n",
        "                   hidden_size=20,\n",
        "                   num_layers=2,\n",
        "                   bidirectional=False,\n",
        "                   )\n",
        "\n",
        "# Custom h0: init h\n",
        "# D: 1 if bidirectional=False else 2\n",
        "h0 = torch.randn(1*2, 1, 20) # (D∗num_layers, N, H_out)\n",
        "output, hn = rnn(inputs, h0)\n",
        "\n",
        "# h0: zero\n",
        "# output, hn = rnn(inputs)\n",
        "\n",
        "output.shape # (L, N, H_out)"
      ],
      "metadata": {
        "id": "lNQCN9dmR4k5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LSTM, GRU Cell"
      ],
      "metadata": {
        "id": "zHeKGtlOzV_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnn = nn.LSTMCell(10, 20)  # (input_size, hidden_size)\n",
        "input = torch.randn(5, 1, 10)  # (time_steps, batch, input_size)\n",
        "hn = torch.randn(1, 20)  # (batch, hidden_size)\n",
        "cn = torch.randn(1, 20)\n",
        "output = []\n",
        "\n",
        "# iterate all time steps\n",
        "for i in range(input.size()[0]):\n",
        "    hn, cn = rnn(input[i], (hn, cn))\n",
        "    output.append(hn)\n",
        "\n",
        "output = torch.stack(output, dim=0)\n",
        "output.shape"
      ],
      "metadata": {
        "id": "XMEZ5v7Ujf8o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}